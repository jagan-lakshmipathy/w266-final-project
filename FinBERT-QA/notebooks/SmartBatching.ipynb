{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1352,"status":"ok","timestamp":1657586045187,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"KgRB1qa3jiqd","outputId":"df9afc76-d875-46f2-d987-a472d65a31f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n","/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1657586045187,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"2kcYzQ1KjvI4","outputId":"6139d48c-e2d2-431e-ea9f-911dd4fb80e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA\n"]}],"source":["%cd /gdrive/MyDrive/nlp-yuan_code/FinBERT-QA"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1940,"status":"ok","timestamp":1657586047125,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"9n4SyChJjyHx","outputId":"e5bc9c2d-d0df-4f04-d45c-bd1ad63c73ab"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["from statistics import mean \n","\n","from src.process_data import *"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16292,"status":"ok","timestamp":1657586063413,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"eGLljwlTj44m","outputId":"c924f075-f3d2-4bc9-be2b-a6e33e08d2ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pyserini in /usr/local/lib/python3.7/dist-packages (0.17.0)\n","Requirement already satisfied: pyjnius>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from pyserini) (1.4.2)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pyserini) (1.4.1)\n","Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.7/dist-packages (from pyserini) (1.21.6)\n","Requirement already satisfied: onnxruntime>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from pyserini) (1.11.1)\n","Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from pyserini) (1.0.2)\n","Requirement already satisfied: lightgbm>=3.3.2 in /usr/local/lib/python3.7/dist-packages (from pyserini) (3.3.2)\n","Requirement already satisfied: sentencepiece>=0.1.95 in /usr/local/lib/python3.7/dist-packages (from pyserini) (0.1.96)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pyserini) (4.64.0)\n","Requirement already satisfied: spacy>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from pyserini) (3.3.1)\n","Requirement already satisfied: nmslib>=2.1.1 in /usr/local/lib/python3.7/dist-packages (from pyserini) (2.1.1)\n","Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from pyserini) (1.3.5)\n","Requirement already satisfied: transformers>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from pyserini) (4.20.1)\n","Requirement already satisfied: Cython>=0.29.21 in /usr/local/lib/python3.7/dist-packages (from pyserini) (0.29.30)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm>=3.3.2->pyserini) (0.37.1)\n","Requirement already satisfied: pybind11<2.6.2 in /usr/local/lib/python3.7/dist-packages (from nmslib>=2.1.1->pyserini) (2.6.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from nmslib>=2.1.1->pyserini) (5.4.8)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime>=1.8.1->pyserini) (3.17.3)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime>=1.8.1->pyserini) (1.12)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->pyserini) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->pyserini) (2022.1)\n","Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from pyjnius>=1.4.0->pyserini) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->pyserini) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->pyserini) (3.1.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (2.11.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (2.0.6)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (2.23.0)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (0.6.1)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (1.0.7)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (0.7.7)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (3.3.0)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (1.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (21.3)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (3.0.6)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (3.0.9)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (4.1.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (0.9.1)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (1.8.2)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (2.0.7)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (57.4.0)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (0.4.1)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (2.4.3)\n","Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (8.0.17)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.2.1->pyserini) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=3.2.1->pyserini) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=3.2.1->pyserini) (5.2.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (1.24.3)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->pyserini) (0.12.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->pyserini) (0.8.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->pyserini) (4.11.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->pyserini) (3.7.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->pyserini) (2022.6.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->pyserini) (6.0)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=3.2.1->pyserini) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.2.1->pyserini) (2.0.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: faiss in /usr/local/lib/python3.7/dist-packages (1.5.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from faiss) (1.21.6)\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","libomp-dev is already the newest version (5.0.1-1).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 62 not upgraded.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: faiss in /usr/local/lib/python3.7/dist-packages (1.5.3)\n","Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.7/dist-packages (1.7.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from faiss) (1.21.6)\n"]}],"source":["!pip install pyserini\n","!pip install faiss\n","\n","!apt install libomp-dev\n","!python -m pip install --upgrade faiss faiss-gpu\n","import faiss"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":4348,"status":"ok","timestamp":1657586067757,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"wWDFPNn0j8cw"},"outputs":[],"source":["#from pyserini.search import SimpleSearcher\n","\n","import pandas as pd\n","import regex as re\n","import csv\n","from itertools import islice\n","import pickle\n","import numpy as np\n","import json\n","import os\n","import sys\n","import argparse\n","from pathlib import Path\n","from sklearn.model_selection import train_test_split\n","from pathlib import Path\n","# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","#from pyserini.search as pysearch\n","\n","from pyserini.search import SimpleSearcher\n","\n","from src.utils import *\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":39287,"status":"ok","timestamp":1657586107040,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"WbnogQG0k5P0"},"outputs":[],"source":["!pip install pydot --quiet\n","!pip install gensim==3.8.3 --quiet\n","!pip install tensorflow-datasets --quiet\n","!pip install -U tensorflow-text --quiet\n","!pip install transformers --quiet\n","!pip install pydot --quiet"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":5959,"status":"ok","timestamp":1657586112975,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"a7dQiv3s9eN0"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","from tensorflow.keras.layers import Embedding, Input, Dense, Lambda\n","from tensorflow.keras.models import Model\n","import tensorflow.keras.backend as K\n","import tensorflow_datasets as tfds\n","import tensorflow_text as tf_text\n","\n","from transformers import BertTokenizer, TFBertModel\n","\n","\n","import sklearn as sk\n","import nltk\n","from nltk.corpus import reuters\n","from nltk.data import find\n","\n","import matplotlib.pyplot as plt\n","\n","import re"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":1794,"status":"ok","timestamp":1657586114766,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"mcUFp4qSj1-M"},"outputs":[],"source":["data_path = '/gdrive/MyDrive/nlp-data/nlp-qa-datasets/FiQA/FiQA_train_task2/'\n","# Document id and Answer text\n","collection = load_answers_to_df(data_path+\"FiQA_train_doc_final.tsv\")\n","# Question id and Question text\n","queries = load_questions_to_df(data_path+\"FiQA_train_question_final.tsv\")\n","# Question id and Answer id pair\n","qid_docid = load_qid_docid_to_df(data_path+\"FiQA_train_question_doc_final.tsv\")"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1657586114767,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"iuKcwlkvkCVf"},"outputs":[],"source":["fiqa_index = \"/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/retriever/lucene-index-fiqa/\"\n","\n","\n","def split_label(qid_docid):\n","    \"\"\"\n","    Split question answer pairs into train, test, validation sets.\n","\n","    Returns:\n","        train_label: Dictonary\n","            key - question id\n","            value - list of relevant docids\n","        test_label: Dictonary\n","            key - question id\n","            value - list of relevant docids\n","        valid_label: Dictonary\n","            key - question id\n","            value - list of relevant docids\n","    ----------\n","    Arguments:\n","        qid_docid: Dataframe containing the question id and relevant docids\n","    \"\"\"\n","    # Group the answers for each question into a list\n","    qid_docid = qid_docid.groupby(['qid']).agg(lambda x: tuple(x)).applymap(list).reset_index()\n","    # Split data\n","    train, test_set = train_test_split(qid_docid, test_size=0.05)\n","    train_set, valid_set = train_test_split(train, test_size=0.1)\n","    # Expand the list of docids into individual rows to represent a single sample\n","    train_data = train_set.explode('docid')\n","    test_data = test_set.explode('docid')\n","    valid_data = valid_set.explode('docid')\n","\n","    # Convert data into dictionary - key: qid, value: list of relevant docid\n","    train_label = label_to_dict(train_data)\n","    test_label = label_to_dict(test_data)\n","    valid_label = label_to_dict(valid_data)\n","\n","    return train_label, test_label, valid_label\n","\n","def split_question(train_label, test_label, valid_label, queries):\n","    \"\"\"\n","    Split questions into train, test, validation sets.\n","\n","    Returns:\n","        train_questions: Dataframe with qids\n","        test_questions: Dataframe with qids\n","        valid_questions: Dataframe with qids\n","    ----------\n","    Arguments:\n","        train_label: Dictionary contraining qid and list of relevant docid\n","        test_label: Dictionary contraining qid and list of relevant docid\n","        valid_label: Dictionary contraining qid and list of relevant docid\n","        queries: Dataframe containing the question id and question text\n","    \"\"\"\n","    # Get a list of question ids\n","    train_q = list(train_label.keys())\n","    test_q = list(test_label.keys())\n","    valid_q = list(valid_label.keys())\n","\n","    # Split question dataframe into train, test, valid set\n","    train_questions = queries[queries['qid'].isin(train_q)]\n","    test_questions = queries[queries['qid'].isin(test_q)]\n","    valid_questions = queries[queries['qid'].isin(valid_q)]\n","\n","    return train_questions, test_questions, valid_questions\n","\n","def split_label(qid_docid):\n","    \"\"\"\n","    Split question answer pairs into train, test, validation sets.\n","\n","    Returns:\n","        train_label: Dictonary\n","            key - question id\n","            value - list of relevant docids\n","        test_label: Dictonary\n","            key - question id\n","            value - list of relevant docids\n","        valid_label: Dictonary\n","            key - question id\n","            value - list of relevant docids\n","    ----------\n","    Arguments:\n","        qid_docid: Dataframe containing the question id and relevant docids\n","    \"\"\"\n","    # Group the answers for each question into a list\n","    qid_docid = qid_docid.groupby(['qid']).agg(lambda x: tuple(x)).applymap(list).reset_index()\n","    # Split data\n","    train, test_set = train_test_split(qid_docid, test_size=0.05)\n","    train_set, valid_set = train_test_split(train, test_size=0.1)\n","    # Expand the list of docids into individual rows to represent a single sample\n","    train_data = train_set.explode('docid')\n","    test_data = test_set.explode('docid')\n","    valid_data = valid_set.explode('docid')\n","\n","    # Convert data into dictionary - key: qid, value: list of relevant docid\n","    train_label = label_to_dict(train_data)\n","    test_label = label_to_dict(test_data)\n","    valid_label = label_to_dict(valid_data)\n","\n","    return train_label, test_label, valid_label\n","\n","def split_question(train_label, test_label, valid_label, queries):\n","    \"\"\"\n","    Split questions into train, test, validation sets.\n","\n","    Returns:\n","        train_questions: Dataframe with qids\n","        test_questions: Dataframe with qids\n","        valid_questions: Dataframe with qids\n","    ----------\n","    Arguments:\n","        train_label: Dictionary contraining qid and list of relevant docid\n","        test_label: Dictionary contraining qid and list of relevant docid\n","        valid_label: Dictionary contraining qid and list of relevant docid\n","        queries: Dataframe containing the question id and question text\n","    \"\"\"\n","    # Get a list of question ids\n","    train_q = list(train_label.keys())\n","    test_q = list(test_label.keys())\n","    valid_q = list(valid_label.keys())\n","\n","    # Split question dataframe into train, test, valid set\n","    train_questions = queries[queries['qid'].isin(train_q)]\n","    test_questions = queries[queries['qid'].isin(test_q)]\n","    valid_questions = queries[queries['qid'].isin(valid_q)]\n","\n","    return train_questions, test_questions, valid_questions\n","\n","def create_dataset(question_df, labels, cands_size):\n","    \"\"\"Retrieves the top-k candidate answers for a question and\n","    creates a list of lists of the dataset containing the question id,\n","    list of relevant answer ids, and the list of answer candidates\n","\n","    Returns:\n","        dataset: list of list in the form [qid, [pos ans], [ans candidates]]\n","    ----------\n","    Arguments:\n","        question_df: Dataframe containing the qid and question text\n","        labels: Dictonary containing the qid to text map\n","        cands_size: int - number of candidates to retrieve\n","    \"\"\"\n","    dataset = []\n","    # Calls retriever\n","    searcher = SimpleSearcher(fiqa_index)\n","    # For each question\n","    for i, row in question_df.iterrows():\n","        qid = row['qid']\n","        tmp = []\n","        # Append qid\n","        tmp.append(qid)\n","        # Append list of relevant docs\n","        tmp.append(labels[qid])\n","        # Retrieves answer candidates\n","        cands = []\n","        query = row['question']\n","        query = re.sub('[£€§]', '', query)\n","        hits = searcher.search(query, k=cands_size)\n","\n","        for docid in range(0, len(hits)):\n","            cands.append(int(hits[docid].docid))\n","        # Append candidate answers\n","        tmp.append(cands)\n","        dataset.append(tmp)\n","\n","    return dataset\n","\n","def get_dataset(query_path, labels_path, cands_size):\n","    \"\"\"Splits the dataset into train, validation, and test set and creates\n","    the dataset form for training, validation, and testing.\n","\n","    Returns:\n","        train_set: list of list in the form [qid, [pos ans], [ans candidates]]\n","        valid_set: list of list in the form [qid, [pos ans], [ans candidates]]\n","        test_set: list of list in the form [qid, [pos ans], [ans candidates]]\n","    ----------\n","    Arguments:\n","        query_path: str - path containing a list of qid and questions\n","        labels_path: str - path containing a list of qid and relevant docid\n","        cands_size: int - number of candidates to retrieve\n","    \"\"\"\n","    # Question id and Question text\n","    queries = load_questions_to_df(query_path)\n","    # Question id and Answer id pair\n","    qid_docid = load_qid_docid_to_df(labels_path)\n","    # qid to docid label map\n","    labels = label_to_dict(qid_docid)\n","    train_label, test_label, valid_label = split_label(qid_docid)\n","    # Split Questions\n","    train_questions, test_questions, \\\n","    valid_questions = split_question(train_label, test_label, valid_label, queries)\n","\n","    print(\"\\nGenerating training set...\\n\")\n","    train_set = create_dataset(train_questions, labels, cands_size)\n","    print(\"Generating validation set...\\n\")\n","    valid_set = create_dataset(valid_questions, labels, cands_size)\n","    print(\"Generating test set...\\n\")\n","    test_set = create_dataset(test_questions, labels, cands_size)\n","\n","    return train_set, valid_set, test_set\n","\n","def main():\n","\n","    parser = argparse.ArgumentParser()\n","    # Required parameters\n","    parser.add_argument(\"--query_path\", default=None, type=str, required=True,\n","    help=\"Path to the question id to text data in .tsv format. Each line should have at least two columns named (qid, question) separated by tab\")\n","    parser.add_argument(\"--label_path\", default=None, type=str, required=True,\n","    help=\"Path to the question id and answer id data in .tsv format. Each line should have at two columns named (qid, docid) separated by tab\")\n","\n","    # Optional parameters\n","    parser.add_argument(\"--cands_size\", default=50, type=int, required=False,\n","    help=\"Number of candidates to retrieve per question.\")\n","    parser.add_argument(\"--output_dir\", default=Path.cwd()/'data/data_pickle/',\n","    type=str, required=False, help=\"The output directory where the generated data will be stored.\")\n","\n","    args = parser.parse_args()\n","\n","    if len(sys.argv) < 4:\n","        print(\"Usage: python3 src/generate_data.py <query_path> <label_path>\")\n","        sys.exit()\n","\n","    train_set, valid_set, test_set = get_dataset(args.query_path, \\\n","                                                 args.label_path, \\\n","                                                 args.cands_size)\n","\n","    save_pickle(args.output_dir + \"train_set.pickle\", train_set)\n","    save_pickle(args.output_dir + \"valid_set.pickle\", valid_set)\n","    save_pickle(args.output_dir + \"test_set.pickle\", test_set)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":75298,"status":"ok","timestamp":1657586190059,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"sAv5KudXkF-o","outputId":"6dbb8008-32f3-4dc4-82bc-a96b4676af59"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Generating training set...\n","\n","SimpleSearcher class has been deprecated, please use LuceneSearcher from pyserini.search.lucene instead\n","Generating validation set...\n","\n","SimpleSearcher class has been deprecated, please use LuceneSearcher from pyserini.search.lucene instead\n","Generating test set...\n","\n","SimpleSearcher class has been deprecated, please use LuceneSearcher from pyserini.search.lucene instead\n"]}],"source":["query_path = \"/gdrive/MyDrive/nlp-data/nlp-qa-datasets/FiQA/FiQA_train_task2/FiQA_train_question_final.tsv\"\n","labels_path = \"/gdrive/MyDrive/nlp-data/nlp-qa-datasets/FiQA/FiQA_train_task2/FiQA_train_question_doc_final.tsv\"\n","train_set, valid_set, test_set = get_dataset(query_path, labels_path, 50)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3893,"status":"ok","timestamp":1657586193930,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"ig4qsYGrkbsG","outputId":"bbddc75b-8d0f-428b-e204-c0f14da0184f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of answers after cleaning: 57600\n","Number of QA pairs after cleaning: 17072\n"]}],"source":["# Cleaning data\n","empty_docs, empty_id = get_empty_docs(collection)\n","# Remove empty answers from collection of answers\n","collection_cleaned = collection.drop(empty_id)\n","# Remove empty answers from qa pairs\n","qid_docid = qid_docid[~qid_docid['docid'].isin(empty_docs)]\n","\n","print(\"Number of answers after cleaning: {}\".format(len(collection_cleaned)))\n","print(\"Number of QA pairs after cleaning: {}\".format(len(qid_docid)))"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":7816,"status":"ok","timestamp":1657586201741,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"VZrf0ETEkrng"},"outputs":[],"source":["processed_answers = process_answers(collection_cleaned)\n","processed_questions = process_questions(queries)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8064,"status":"ok","timestamp":1657586209785,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"gl4OhiyakwAr","outputId":"b71bd116-e6b2-4157-ff29-1489202559d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Vocab size: 85034\n","Top 35 common words: [('the', 371203), ('to', 233559), ('a', 201620), ('you', 166702), ('and', 163066), ('of', 157574), ('is', 129894), ('in', 120019), ('that', 111416), ('for', 89366), ('it', 83822), ('i', 74100), ('your', 68153), ('are', 67255), ('if', 60689), ('be', 59266), ('on', 58382), ('have', 55754), ('as', 50088), ('this', 49868), ('not', 49227), ('or', 46080), ('with', 45894), ('they', 44485), ('but', 41690), ('can', 38863), ('will', 36865), ('at', 35548), ('an', 31392), ('money', 31003), ('so', 29980), ('$', 29096), ('would', 28750), ('from', 28582), ('more', 27378)]\n"]}],"source":["word2index, word2count = create_vocab(processed_answers, processed_questions)\n","\n","print(\"Vocab size: {}\".format(len(word2index)))\n","print(\"Top {} common words: {}\".format(35, Counter(word2count).most_common(35)))"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":8984,"status":"ok","timestamp":1657586218748,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"H5IfRvGlk05a"},"outputs":[],"source":["qid_to_text, docid_to_text = id_to_text(collection, queries)\n","qid_to_tokenized_text, docid_to_tokenized_text = id_to_tokenized_text(processed_answers, processed_questions)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4196,"status":"ok","timestamp":1657586222926,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"pdJ99CWxlTi-","outputId":"082699b4-6175-4e99-b390-1259f380c979"},"outputs":[{"output_type":"stream","name":"stdout","text":["49979 49979 49979 49979\n"]}],"source":["id1 = load_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/transient/' + \"train_input_ids1.pickle\")\n","type1 = load_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/transient/' + \"train_token_type_ids1.pickle\")\n","mask1 = load_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/transient/' + \"train_att_masks1.pickle\")\n","label1 = load_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/transient/' + \"train_labels1.pickle\")\n","print(len(id1), len(type1), len(mask1), len(label1))"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":8516,"status":"ok","timestamp":1657586231422,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"9GqmikoRht0C"},"outputs":[],"source":["!pip install -q tensorflow-recommenders\n","!pip install -q scann"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1657586231423,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"q129GNAJhnCM"},"outputs":[],"source":["import os\n","import pprint\n","import tempfile\n","\n","from typing import Dict, Text\n","\n","import numpy as np\n","import tensorflow as tf\n","#import tensorflow_datasets as tfds\n","\n","import tensorflow_recommenders as tfrs"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1657586231423,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"eIAV_vf_kREV"},"outputs":[],"source":["embedding_dimension = 32"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1657586231423,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"DJUA_3cf3I3i"},"outputs":[],"source":["def generate_ids(data):\n","  qids = []\n","  aids = []\n","  qa_pairs = []\n","\n","\n","  #for i, seq in enumerate(tqdm(train_set)):\n","  for i, seq in enumerate(data):\n","    qid, ans_labels, cands = seq[0], seq[1], seq[2]\n","    #print (qid, ans_labels, cands)\n","    \n","\n","    # For each answer in the candidates\n","    for docid in ans_labels:\n","      qids.append(qid)\n","      aids.append(docid)\n","      qa_pairs.append((qid, docid))\n","\n","  unique_qids = np.unique(qids)\n","  unique_aids = np.unique(aids)\n","  return unique_qids, unique_aids, qa_pairs\n","      \n","qids, aids, qa_pairs = generate_ids(train_set)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1657586231423,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"2-ZRFYZ3zbvr","outputId":"02eb8fc4-89dc-40f1-dc54-c65afca3b7b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["5683 14661 14661\n"]},{"output_type":"execute_result","data":{"text/plain":["numpy.ndarray"]},"metadata":{},"execution_count":20}],"source":["print(len(qids), len(aids), len(qa_pairs))\n","type(qids)"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":254,"status":"ok","timestamp":1657586733756,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"O5o2DxxT3dPH"},"outputs":[],"source":["unique_questions = np.unique([qid_to_text[qid] for qid in qids])\n","\n","\n","question_model = tf.keras.Sequential([\n","  tf.keras.layers.StringLookup(\n","      vocabulary=unique_questions, mask_token=None),\n","  # We add an additional embedding to account for unknown tokens.\n","  tf.keras.layers.Embedding(len(qids) + 1, embedding_dimension)\n","])"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":6760,"status":"ok","timestamp":1657586742844,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"dMPIGeLP7Jd9"},"outputs":[],"source":["unique_answers = np.unique([docid_to_text[aid] for aid in aids])\n","\n","answer_model = tf.keras.Sequential([\n","  tf.keras.layers.StringLookup(\n","      vocabulary=unique_answers, mask_token=None),\n","  # We add an additional embedding to account for unknown tokens.\n","  tf.keras.layers.Embedding(len(aids) + 1, embedding_dimension)\n","])"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":716,"status":"ok","timestamp":1657586758666,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"gpS-kfrt7jXz"},"outputs":[],"source":["answers = tf.data.Dataset.from_tensor_slices([docid_to_text[aid] if isinstance(docid_to_text.get(aid),str) else '' for aid in aids])\n","\n","metrics = tfrs.metrics.FactorizedTopK(\n","  candidates=answers.batch(128).map(answer_model)\n",")\n","task = tfrs.tasks.Retrieval(\n","  metrics=metrics\n",")"]},{"cell_type":"code","source":["questions = tf.data.Dataset.from_tensor_slices([qid_to_text[qid] if isinstance(qid_to_text.get(qid),str) else '' for qid, aid in qa_pairs])\n","answers = tf.data.Dataset.from_tensor_slices([docid_to_text[aid] if isinstance(docid_to_text.get(aid),str) else '' for qid, aid in qa_pairs])\n","\n","\n","ds = tf.data.Dataset.zip((questions, answers))\n","ds = ds.map(lambda x, y : {\"question\": x, \"answer\": y})\n","\n","tf.random.set_seed(42)\n","shuffled = ds.shuffle(14_661, seed=42, reshuffle_each_iteration=False)\n","\n","train = shuffled.take(12_000)\n","test = shuffled.skip(12_000).take(2_661)"],"metadata":{"id":"FgF4Rq-a-y2z","executionInfo":{"status":"ok","timestamp":1657589024728,"user_tz":240,"elapsed":520,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["len(train), len(test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rnh4pCvM_nwG","executionInfo":{"status":"ok","timestamp":1657589035513,"user_tz":240,"elapsed":283,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"}},"outputId":"b5f08323-becb-4f0b-d618-e8e557b0332f"},"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(12000, 2661)"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","execution_count":50,"metadata":{"executionInfo":{"elapsed":258,"status":"ok","timestamp":1657589047865,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"R9XsyaKS7Uih"},"outputs":[],"source":["class QAModel(tfrs.Model):\n","\n","  def __init__(self, question_model, answer_model):\n","    super().__init__()\n","    self.question_model: tf.keras.Model = question_model\n","    self.answer_model: tf.keras.Model = answer_model\n","    self.task: tf.keras.layers.Layer = task\n","\n","  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n","    # We pick out the user features and pass them into the user model.\n","    q_embeddings = self.question_model(features[\"question\"])\n","    # And pick out the movie features and pass them into the movie model,\n","    # getting embeddings back.\n","    a_embeddings = self.answer_model(features[\"answer\"])\n","\n","    # The task computes the loss and the metrics.\n","    return self.task(q_embeddings, a_embeddings)"]},{"cell_type":"code","source":["cached_train = train.shuffle(12000).batch(1000).cache()"],"metadata":{"id":"hK-TL1Dv4Hkp","executionInfo":{"status":"ok","timestamp":1657589078265,"user_tz":240,"elapsed":275,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["model = QAModel(question_model, answer_model)\n","model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n","\n","\n","model.fit(cached_train, epochs=3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hRT_aQpbCAJ6","executionInfo":{"status":"ok","timestamp":1657589145577,"user_tz":240,"elapsed":64246,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"}},"outputId":"e92e763e-00f9-497c-a978-1e62a86b4fe7"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","12/12 [==============================] - 22s 2s/step - factorized_top_k/top_1_categorical_accuracy: 8.3333e-04 - factorized_top_k/top_5_categorical_accuracy: 0.6845 - factorized_top_k/top_10_categorical_accuracy: 0.8884 - factorized_top_k/top_50_categorical_accuracy: 0.9859 - factorized_top_k/top_100_categorical_accuracy: 0.9983 - loss: 2334.0143 - regularization_loss: 0.0000e+00 - total_loss: 2334.0143\n","Epoch 2/3\n","12/12 [==============================] - 21s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0097 - factorized_top_k/top_5_categorical_accuracy: 0.8293 - factorized_top_k/top_10_categorical_accuracy: 0.9544 - factorized_top_k/top_50_categorical_accuracy: 0.9992 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1669.8356 - regularization_loss: 0.0000e+00 - total_loss: 1669.8356\n","Epoch 3/3\n","12/12 [==============================] - 21s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0259 - factorized_top_k/top_5_categorical_accuracy: 0.8726 - factorized_top_k/top_10_categorical_accuracy: 0.9732 - factorized_top_k/top_50_categorical_accuracy: 0.9999 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1264.2281 - regularization_loss: 0.0000e+00 - total_loss: 1264.2281\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fe05d6f11d0>"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["cached_test = test.batch(2661).cache()\n","model.evaluate(cached_test, return_dict=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FmtUd_V_CE6m","executionInfo":{"status":"ok","timestamp":1657589316766,"user_tz":240,"elapsed":5464,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"}},"outputId":"8b70b59b-a02d-4b89-84b6-51da5bf7e591"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 5s 5s/step - factorized_top_k/top_1_categorical_accuracy: 0.0105 - factorized_top_k/top_5_categorical_accuracy: 0.6077 - factorized_top_k/top_10_categorical_accuracy: 0.8207 - factorized_top_k/top_50_categorical_accuracy: 0.9309 - factorized_top_k/top_100_categorical_accuracy: 0.9733 - loss: 7191.8521 - regularization_loss: 0.0000e+00 - total_loss: 7191.8521\n"]},{"output_type":"execute_result","data":{"text/plain":["{'factorized_top_k/top_100_categorical_accuracy': 0.9733182787895203,\n"," 'factorized_top_k/top_10_categorical_accuracy': 0.8207440972328186,\n"," 'factorized_top_k/top_1_categorical_accuracy': 0.010522359982132912,\n"," 'factorized_top_k/top_50_categorical_accuracy': 0.9308530688285828,\n"," 'factorized_top_k/top_5_categorical_accuracy': 0.6076663136482239,\n"," 'loss': 7191.85205078125,\n"," 'regularization_loss': 0,\n"," 'total_loss': 7191.85205078125}"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["scann_index = tfrs.layers.factorized_top_k.ScaNN(model.question_model)\n","scann_index.index_from_dataset(\n","  tf.data.Dataset.zip((answers.batch(100), answers.batch(100).map(model.answer_model)))\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hZSgeFg1DUNB","executionInfo":{"status":"ok","timestamp":1657590415771,"user_tz":240,"elapsed":1044,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"}},"outputId":"1367cac8-9a9e-4b3a-ef85-ddf0ab67af36"},"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow_recommenders.layers.factorized_top_k.ScaNN at 0x7fe05d5f3d10>"]},"metadata":{},"execution_count":61}]},{"cell_type":"code","source":["# Get recommendations.\n","_, cands = scann_index(tf.constant([qid_to_text[5]]))\n","\n","print(f'QUESTION: ', qid_to_text[5])\n","print(f\"Recommendations for user 42: {cands[0, :10]}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EXMDEbfOF1E6","executionInfo":{"status":"ok","timestamp":1657590683073,"user_tz":240,"elapsed":278,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"}},"outputId":"7de8f5a4-2588-4892-f120-3135f349d7e2"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["QUESTION:  Starting a new online business\n","Recommendations for user 42: [b\"This could be a case of the new chip card technology and dealing with slow reimbursement turnaround time. I recently visited a restaurant who was not using the chip technology, and it refused my card after several attempts. I found out from my bank it was because the restaurant was not set up for chip and I had not eaten there before....I know at the other end it takes far longer for the funds to get to the merchant; banks don't want to part with other people's money.\"\n"," b'Why would such a large discount make business sense to the restaurant? The legit reasons could be;   Or can I assume that the restaurant is trying to avoid leaving a paper trail so that they could avoid paying tax? The illegal reasons could be;'\n"," b'To expand on keshlam\\'s answer: A direct feed does not involve a website of any kind.  Each exchange publishes its order/trade feed(s) onto a packet network where subscribers have machines listening and reacting. Let\\'s call the moment when a trade occurs inside an exchange\\'s matching engine \"T0\".  An exchange then publishes the specifics of that trade as above, and the moment when that information is first available to subscribers is T1.  In some cases, T1 - T0 is a few microseconds; in other (notorious) cases, it can be as much as 100 milliseconds (100,000x longer). Because it\\'s expensive for a subscriber to run a machine on each exchange\\'s network -- and also because it requires a team of engineers devoted to understanding each exchange\\'s individual publication protocols -- it seems unlikely that Google pays for direct access.  Instead Google most likely pays another company who is a subscriber on each exchange around the world (let\\'s say Reuters) to forward their incoming information to Google.  Reuters then charges Google and other customers according to how fast the customer wants the forwarded information.  Reuters has to parse the info it gets at T1, check it for errors, and translate it into a format that Google (and other customers) can understand.  Let\\'s say they finish all that work and put their new packets on the internet at time T2. Then the slow crawl across the internet begins.  Some 5-100 milliseconds later your website of choice gets its pre-processed data at time T3.  Even though it\\'s preprocessed, your favorite website has to unpack the data, store it in some sort of database, and push it onto their website at time T4. A sophisticated website might then force a refresh of your browser at time T4 to show you the new information.  But this forced refresh involves yet another slow crawl across the internet from where your website is based to your home computer, competing with your neighbor\\'s 24/7 Netflix stream, etc.  Then your browser (with its 83 plugins and banner ads everywhere) has to refresh, and you finally see the update at T5. So, a thousand factors come into play, but even assuming that Google is doing the most expensive and labor-intensive thing it can and that all the networks between you and Google and the exchange are as short as they can be, you\\'re not going to hear about a trade -- even a massive, market-moving trade -- for anywhere from 500 milliseconds to 5 seconds after T0.  And in a more realistic world that time will be 10-30 seconds. This is what Google calls \"Realtime\" on that disclaimer page, because they feel they\\'re getting that info to you as fast as they possibly can (for free). Meanwhile, the computers that actually subscribe to an exchange heard about the trade way back at time T1 and acted on that information in a few microseconds.  That\\'s almost certainly before T2 and definitely way way before T3.  The market for a particular instrument could change direction 5 times before Google even shows the first trade. So if you want true realtime access, you must subscribe to the exchange feed or, as keshlam suggests, sign up with a broker that provides its own optimized market feeds to you. (Note: This is not an endorsement of trading through brokers.)'\n"," b'check the DATE OF SERVICE on all your invoices carefully.  It\\'s possible you actually DID pay already. Sometimes when a medical provider gets \"mostly\" paid by a third party insurer, they just drop the (small) remainder, as it\\'s more cost than it\\'s worth if it is a trivial amount.  Alternatively, they wait until you show up for another office visit, and \"ding\" you then!'\n"," b'In addition to the company-specific annual business cycle reasons and company-specific historical reasons mentioned in the other answers, there is another reason. Accounting firms tend to be very busy during January (and February and March) when most companies are closing and auditing their calendar-year books.  If a company chooses its fiscal year to end at a different time of year, the accounting firms are more available, and the auditing costs might be lower.'\n"," b\"No there aren't any such funds.\"\n"," b'I would call the bank and ask how the person is on the account.   If they are an owner, or are an authorized user, or what type of owner they are, etc.    If the bank makes the distinction between \"user\" and \"owner\" then most likely, your funds are not able to be seized.  If they are a joint owner, then, typically, 100% of the money is yours and 100% of the money is theirs and either of you could withdraw all the money, close the account, or have the money seized as part of a legal action.'\n"," b\"The main source is a direct feed from the stock market itself. The faster the feed, the more expensive. 15-minute delay is essentially free... and for those of us who do long-term investment is more than adequate.  If you want data sooner, sign up with a brokerage that provides that service as part of what you're paying them for... and remember that every bit you spend on services is that much more profit you have to make just to break even, so there's a real tradeoff.\"\n"," b'Each bank is different, so your question needs to be more specific. For instance, I believe Paypal and Chase settles at 7pm EST on business days. Bank of America at 5PM.'\n"," b'Im not sure if its normal/sensical/healthy, and that is kind of opinion based. But there is a reason for it. Certain rules and regulations passed recently are causing companies or institutions to shift to bonds from cash. Fidelity, for example, is completely converting its $100 billion dollar cash fund to short term bills. Its estimated that over $2 trillion that is now in cash may be converted to bills, and that will obviously put upward preasure on the price of them. The treasury is trying to issue more short term debt to balance out the demand. read more here: http://www.wsj.com/articles/money-funds-clamor-for-short-term-treasurys-1445300813']\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Ju2jYFTUvIu"},"outputs":[],"source":["import math\n","bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n","def generate_bert_params(ids, m):\n","  input_ids = []\n","  token_type_ids = []\n","  att_masks = []\n","\n","  for id in ids:\n","    if m.get(id) and isinstance(m.get(id),str):\n","      encoded_seq = bert_tokenizer.encode_plus(m[id][:500],\n","                                              max_length=512,\n","                                              pad_to_max_length=True,\n","                                              return_token_type_ids=True,\n","                                              truncation=True,\n","                                              return_attention_mask = True)\n","      input_ids.append(encoded_seq['input_ids'])\n","      token_type_ids.append(encoded_seq['token_type_ids'])\n","      att_masks.append(encoded_seq['attention_mask'])\n","  return input_ids, token_type_ids, att_masks"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2783,"status":"ok","timestamp":1657501126023,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"zezKAGozfmO4","outputId":"66bab1b5-31e1-4468-ce70-70fecf847d5f"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}],"source":["qp_ids, qp_types, qp_masks = generate_bert_params(qids, qid_to_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27864,"status":"ok","timestamp":1657501476297,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"4WVSmPNjvKrI","outputId":"0582089d-b66c-4655-d8a0-16a70e53b941"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}],"source":["ap_ids, ap_types, ap_masks = generate_bert_params(aids, docid_to_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":121,"status":"ok","timestamp":1657501550681,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"gWWfm7jYyYeW","outputId":"adf3dd88-bc1c-4a8f-9c6d-9218950065ea"},"outputs":[{"name":"stdout","output_type":"stream","text":["14581 14581 14581 5683 5683 5683\n"]}],"source":["print(len(ap_ids), len(ap_types), len(ap_masks), len(qp_ids), len(qp_types), len(qp_masks))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jCFa9BCLV2jM"},"outputs":[],"source":["embedding_dimension = 512"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0IxziO2p2z0L"},"outputs":[],"source":["def create_bert_model():\n","\n","    bert_model = TFBertModel.from_pretrained('bert-base-cased')\n","    max_length = 512\n","\n","    #restrict training to the train_layers outer transformer layers\n","    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='input_ids_layer') #--SOLUTION--\n","    token_type_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='token_type_ids_layer')\n","    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='attention_mask_layer')\n","\n","\n","    bert_inputs = {'input_ids': input_ids,\n","                   'token_type_ids': token_type_ids,\n","                   'attention_mask': attention_mask}         \n","\n","    x = bert_model(bert_inputs)[0][:,0,:]\n","    return tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs = x)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jz3CjKyTW1vj","outputId":"3924db33-ddc8-4cc1-f3de-201e890e4197"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]}],"source":["q_model = create_bert_model()([qp_ids, qp_types, qp_masks])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":166},"executionInfo":{"elapsed":133,"status":"error","timestamp":1657504578686,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"},"user_tz":240},"id":"lc41_GEJ6qi-","outputId":"fc4e2594-8902-4453-a410-2a33f3f996d3"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-75-532f154bbe59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mq_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'q_model' is not defined"]}],"source":["q_model[0]"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"SmartBatching.ipynb","provenance":[],"authorship_tag":"ABX9TyObyUtK61y/9bkQ38r0ScFz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}