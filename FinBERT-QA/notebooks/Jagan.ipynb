{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Jagan.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyMX9OEp6yg47AL1s9bfr+1v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sT9W_F7dZo-B","executionInfo":{"status":"ok","timestamp":1656723788454,"user_tz":240,"elapsed":655,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"}},"outputId":"2cb99ca4-33ec-477c-dd4b-6d5111ebb12c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n","/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive"]},{"cell_type":"code","source":["%cd /gdrive/MyDrive/nlp-yuan_code/FinBERT-QA"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hqbKb2IfaMev","executionInfo":{"status":"ok","timestamp":1656723788454,"user_tz":240,"elapsed":3,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"}},"outputId":"b6384008-1029-47ed-ec86-f037536f3740"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA\n"]}]},{"cell_type":"code","source":["from statistics import mean \n","\n","from src.process_data import *"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-dt95X49aukG","executionInfo":{"status":"ok","timestamp":1656723792425,"user_tz":240,"elapsed":3973,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"}},"outputId":"646815f7-192b-4923-b24e-efda8c271e03"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["data_path = '/gdrive/MyDrive/nlp-data/nlp-qa-datasets/FiQA/FiQA_train_task2/'\n","# Document id and Answer text\n","collection = load_answers_to_df(data_path+\"FiQA_train_doc_final.tsv\")\n","# Question id and Question text\n","queries = load_questions_to_df(data_path+\"FiQA_train_question_final.tsv\")\n","# Question id and Answer id pair\n","qid_docid = load_qid_docid_to_df(data_path+\"FiQA_train_question_doc_final.tsv\")"],"metadata":{"id":"TZqD1HlyamWD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install pyserini\n","!pip install faiss\n","\n","!apt install libomp-dev\n","!python -m pip install --upgrade faiss faiss-gpu\n","import faiss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xg9tYn8QfLOE","executionInfo":{"status":"ok","timestamp":1656723812175,"user_tz":240,"elapsed":18595,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"}},"outputId":"bc83a434-faac-448d-be66-601e34746abc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pyserini in /usr/local/lib/python3.7/dist-packages (0.17.0)\n","Requirement already satisfied: Cython>=0.29.21 in /usr/local/lib/python3.7/dist-packages (from pyserini) (0.29.30)\n","Requirement already satisfied: transformers>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from pyserini) (4.20.1)\n","Requirement already satisfied: lightgbm>=3.3.2 in /usr/local/lib/python3.7/dist-packages (from pyserini) (3.3.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pyserini) (4.64.0)\n","Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from pyserini) (1.0.2)\n","Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.7/dist-packages (from pyserini) (1.21.6)\n","Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from pyserini) (1.3.5)\n","Requirement already satisfied: pyjnius>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from pyserini) (1.4.1)\n","Requirement already satisfied: sentencepiece>=0.1.95 in /usr/local/lib/python3.7/dist-packages (from pyserini) (0.1.96)\n","Requirement already satisfied: spacy>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from pyserini) (3.3.1)\n","Requirement already satisfied: onnxruntime>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from pyserini) (1.11.1)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pyserini) (1.4.1)\n","Requirement already satisfied: nmslib>=2.1.1 in /usr/local/lib/python3.7/dist-packages (from pyserini) (2.1.1)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm>=3.3.2->pyserini) (0.37.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from nmslib>=2.1.1->pyserini) (5.4.8)\n","Requirement already satisfied: pybind11<2.6.2 in /usr/local/lib/python3.7/dist-packages (from nmslib>=2.1.1->pyserini) (2.6.1)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime>=1.8.1->pyserini) (2.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime>=1.8.1->pyserini) (3.17.3)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->pyserini) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->pyserini) (2022.1)\n","Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from pyjnius>=1.4.0->pyserini) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->pyserini) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->pyserini) (3.1.0)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (0.6.1)\n","Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (8.0.17)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (2.23.0)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (4.1.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (3.3.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (21.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (0.9.1)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (0.4.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (2.0.6)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (3.0.9)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (2.4.3)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (2.0.7)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (1.0.2)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (1.8.2)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (0.7.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (3.0.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (2.11.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (1.0.7)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (57.4.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.2.1->pyserini) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=3.2.1->pyserini) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=3.2.1->pyserini) (5.2.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (1.24.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->pyserini) (6.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->pyserini) (4.11.4)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->pyserini) (0.12.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->pyserini) (2022.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->pyserini) (3.7.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->pyserini) (0.8.1)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=3.2.1->pyserini) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.2.1->pyserini) (2.0.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: faiss in /usr/local/lib/python3.7/dist-packages (1.5.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from faiss) (1.21.6)\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","libomp-dev is already the newest version (5.0.1-1).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: faiss in /usr/local/lib/python3.7/dist-packages (1.5.3)\n","Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.7/dist-packages (1.7.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from faiss) (1.21.6)\n"]}]},{"cell_type":"code","source":["#from pyserini.search import SimpleSearcher\n","\n","import pandas as pd\n","import regex as re\n","import csv\n","from itertools import islice\n","import pickle\n","import numpy as np\n","import json\n","import os\n","import sys\n","import argparse\n","from pathlib import Path\n","from sklearn.model_selection import train_test_split\n","from pathlib import Path\n","# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","#from pyserini.search as pysearch\n","\n","from pyserini.search import SimpleSearcher\n","\n","from src.utils import *"],"metadata":{"id":"NlnN_LjggEI4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fiqa_index = \"/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/retriever/lucene-index-fiqa/\"\n","\n","\n","def split_label(qid_docid):\n","    \"\"\"\n","    Split question answer pairs into train, test, validation sets.\n","\n","    Returns:\n","        train_label: Dictonary\n","            key - question id\n","            value - list of relevant docids\n","        test_label: Dictonary\n","            key - question id\n","            value - list of relevant docids\n","        valid_label: Dictonary\n","            key - question id\n","            value - list of relevant docids\n","    ----------\n","    Arguments:\n","        qid_docid: Dataframe containing the question id and relevant docids\n","    \"\"\"\n","    # Group the answers for each question into a list\n","    qid_docid = qid_docid.groupby(['qid']).agg(lambda x: tuple(x)).applymap(list).reset_index()\n","    # Split data\n","    train, test_set = train_test_split(qid_docid, test_size=0.05)\n","    train_set, valid_set = train_test_split(train, test_size=0.1)\n","    # Expand the list of docids into individual rows to represent a single sample\n","    train_data = train_set.explode('docid')\n","    test_data = test_set.explode('docid')\n","    valid_data = valid_set.explode('docid')\n","\n","    # Convert data into dictionary - key: qid, value: list of relevant docid\n","    train_label = label_to_dict(train_data)\n","    test_label = label_to_dict(test_data)\n","    valid_label = label_to_dict(valid_data)\n","\n","    return train_label, test_label, valid_label\n","\n","def split_question(train_label, test_label, valid_label, queries):\n","    \"\"\"\n","    Split questions into train, test, validation sets.\n","\n","    Returns:\n","        train_questions: Dataframe with qids\n","        test_questions: Dataframe with qids\n","        valid_questions: Dataframe with qids\n","    ----------\n","    Arguments:\n","        train_label: Dictionary contraining qid and list of relevant docid\n","        test_label: Dictionary contraining qid and list of relevant docid\n","        valid_label: Dictionary contraining qid and list of relevant docid\n","        queries: Dataframe containing the question id and question text\n","    \"\"\"\n","    # Get a list of question ids\n","    train_q = list(train_label.keys())\n","    test_q = list(test_label.keys())\n","    valid_q = list(valid_label.keys())\n","\n","    # Split question dataframe into train, test, valid set\n","    train_questions = queries[queries['qid'].isin(train_q)]\n","    test_questions = queries[queries['qid'].isin(test_q)]\n","    valid_questions = queries[queries['qid'].isin(valid_q)]\n","\n","    return train_questions, test_questions, valid_questions\n","\n","def split_label(qid_docid):\n","    \"\"\"\n","    Split question answer pairs into train, test, validation sets.\n","\n","    Returns:\n","        train_label: Dictonary\n","            key - question id\n","            value - list of relevant docids\n","        test_label: Dictonary\n","            key - question id\n","            value - list of relevant docids\n","        valid_label: Dictonary\n","            key - question id\n","            value - list of relevant docids\n","    ----------\n","    Arguments:\n","        qid_docid: Dataframe containing the question id and relevant docids\n","    \"\"\"\n","    # Group the answers for each question into a list\n","    qid_docid = qid_docid.groupby(['qid']).agg(lambda x: tuple(x)).applymap(list).reset_index()\n","    # Split data\n","    train, test_set = train_test_split(qid_docid, test_size=0.05)\n","    train_set, valid_set = train_test_split(train, test_size=0.1)\n","    # Expand the list of docids into individual rows to represent a single sample\n","    train_data = train_set.explode('docid')\n","    test_data = test_set.explode('docid')\n","    valid_data = valid_set.explode('docid')\n","\n","    # Convert data into dictionary - key: qid, value: list of relevant docid\n","    train_label = label_to_dict(train_data)\n","    test_label = label_to_dict(test_data)\n","    valid_label = label_to_dict(valid_data)\n","\n","    return train_label, test_label, valid_label\n","\n","def split_question(train_label, test_label, valid_label, queries):\n","    \"\"\"\n","    Split questions into train, test, validation sets.\n","\n","    Returns:\n","        train_questions: Dataframe with qids\n","        test_questions: Dataframe with qids\n","        valid_questions: Dataframe with qids\n","    ----------\n","    Arguments:\n","        train_label: Dictionary contraining qid and list of relevant docid\n","        test_label: Dictionary contraining qid and list of relevant docid\n","        valid_label: Dictionary contraining qid and list of relevant docid\n","        queries: Dataframe containing the question id and question text\n","    \"\"\"\n","    # Get a list of question ids\n","    train_q = list(train_label.keys())\n","    test_q = list(test_label.keys())\n","    valid_q = list(valid_label.keys())\n","\n","    # Split question dataframe into train, test, valid set\n","    train_questions = queries[queries['qid'].isin(train_q)]\n","    test_questions = queries[queries['qid'].isin(test_q)]\n","    valid_questions = queries[queries['qid'].isin(valid_q)]\n","\n","    return train_questions, test_questions, valid_questions\n","\n","def create_dataset(question_df, labels, cands_size):\n","    \"\"\"Retrieves the top-k candidate answers for a question and\n","    creates a list of lists of the dataset containing the question id,\n","    list of relevant answer ids, and the list of answer candidates\n","\n","    Returns:\n","        dataset: list of list in the form [qid, [pos ans], [ans candidates]]\n","    ----------\n","    Arguments:\n","        question_df: Dataframe containing the qid and question text\n","        labels: Dictonary containing the qid to text map\n","        cands_size: int - number of candidates to retrieve\n","    \"\"\"\n","    dataset = []\n","    # Calls retriever\n","    searcher = SimpleSearcher(fiqa_index)\n","    # For each question\n","    for i, row in question_df.iterrows():\n","        qid = row['qid']\n","        tmp = []\n","        # Append qid\n","        tmp.append(qid)\n","        # Append list of relevant docs\n","        tmp.append(labels[qid])\n","        # Retrieves answer candidates\n","        cands = []\n","        query = row['question']\n","        query = re.sub('[£€§]', '', query)\n","        hits = searcher.search(query, k=cands_size)\n","\n","        for docid in range(0, len(hits)):\n","            cands.append(int(hits[docid].docid))\n","        # Append candidate answers\n","        tmp.append(cands)\n","        dataset.append(tmp)\n","\n","    return dataset\n","\n","def get_dataset(query_path, labels_path, cands_size):\n","    \"\"\"Splits the dataset into train, validation, and test set and creates\n","    the dataset form for training, validation, and testing.\n","\n","    Returns:\n","        train_set: list of list in the form [qid, [pos ans], [ans candidates]]\n","        valid_set: list of list in the form [qid, [pos ans], [ans candidates]]\n","        test_set: list of list in the form [qid, [pos ans], [ans candidates]]\n","    ----------\n","    Arguments:\n","        query_path: str - path containing a list of qid and questions\n","        labels_path: str - path containing a list of qid and relevant docid\n","        cands_size: int - number of candidates to retrieve\n","    \"\"\"\n","    # Question id and Question text\n","    queries = load_questions_to_df(query_path)\n","    # Question id and Answer id pair\n","    qid_docid = load_qid_docid_to_df(labels_path)\n","    # qid to docid label map\n","    labels = label_to_dict(qid_docid)\n","    train_label, test_label, valid_label = split_label(qid_docid)\n","    # Split Questions\n","    train_questions, test_questions, \\\n","    valid_questions = split_question(train_label, test_label, valid_label, queries)\n","\n","    print(\"\\nGenerating training set...\\n\")\n","    train_set = create_dataset(train_questions, labels, cands_size)\n","    print(\"Generating validation set...\\n\")\n","    valid_set = create_dataset(valid_questions, labels, cands_size)\n","    print(\"Generating test set...\\n\")\n","    test_set = create_dataset(test_questions, labels, cands_size)\n","\n","    return train_set, valid_set, test_set\n","\n","def main():\n","\n","    parser = argparse.ArgumentParser()\n","    # Required parameters\n","    parser.add_argument(\"--query_path\", default=None, type=str, required=True,\n","    help=\"Path to the question id to text data in .tsv format. Each line should have at least two columns named (qid, question) separated by tab\")\n","    parser.add_argument(\"--label_path\", default=None, type=str, required=True,\n","    help=\"Path to the question id and answer id data in .tsv format. Each line should have at two columns named (qid, docid) separated by tab\")\n","\n","    # Optional parameters\n","    parser.add_argument(\"--cands_size\", default=50, type=int, required=False,\n","    help=\"Number of candidates to retrieve per question.\")\n","    parser.add_argument(\"--output_dir\", default=Path.cwd()/'data/data_pickle/',\n","    type=str, required=False, help=\"The output directory where the generated data will be stored.\")\n","\n","    args = parser.parse_args()\n","\n","    if len(sys.argv) < 4:\n","        print(\"Usage: python3 src/generate_data.py <query_path> <label_path>\")\n","        sys.exit()\n","\n","    train_set, valid_set, test_set = get_dataset(args.query_path, \\\n","                                                 args.label_path, \\\n","                                                 args.cands_size)\n","\n","    save_pickle(args.output_dir + \"train_set.pickle\", train_set)\n","    save_pickle(args.output_dir + \"valid_set.pickle\", valid_set)\n","    save_pickle(args.output_dir + \"test_set.pickle\", test_set)"],"metadata":{"id":"R9YJhzHufmNj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query_path = \"/gdrive/MyDrive/nlp-data/nlp-qa-datasets/FiQA/FiQA_train_task2/FiQA_train_question_final.tsv\"\n","labels_path = \"/gdrive/MyDrive/nlp-data/nlp-qa-datasets/FiQA/FiQA_train_task2/FiQA_train_question_doc_final.tsv\"\n","train_set, valid_set, test_set = get_dataset(query_path, labels_path, 50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iwakhfkgm0Em","executionInfo":{"status":"ok","timestamp":1656723882089,"user_tz":240,"elapsed":61503,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"}},"outputId":"aafd287b-4c60-426f-cf21-1215a08e4b78"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Generating training set...\n","\n","SimpleSearcher class has been deprecated, please use LuceneSearcher from pyserini.search.lucene instead\n","Generating validation set...\n","\n","SimpleSearcher class has been deprecated, please use LuceneSearcher from pyserini.search.lucene instead\n","Generating test set...\n","\n","SimpleSearcher class has been deprecated, please use LuceneSearcher from pyserini.search.lucene instead\n"]}]},{"cell_type":"code","source":["# Cleaning data\n","empty_docs, empty_id = get_empty_docs(collection)\n","# Remove empty answers from collection of answers\n","collection_cleaned = collection.drop(empty_id)\n","# Remove empty answers from qa pairs\n","qid_docid = qid_docid[~qid_docid['docid'].isin(empty_docs)]\n","\n","print(\"Number of answers after cleaning: {}\".format(len(collection_cleaned)))\n","print(\"Number of QA pairs after cleaning: {}\".format(len(qid_docid)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J0uZM0DCAUVk","executionInfo":{"status":"ok","timestamp":1656723884781,"user_tz":240,"elapsed":2713,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"}},"outputId":"0a6e8f7a-9469-4586-c0f7-56a87bfb4b7e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of answers after cleaning: 57600\n","Number of QA pairs after cleaning: 17072\n"]}]},{"cell_type":"code","source":["# Write collection df to file\n","save_tsv(\"retriever/collection_cleaned.tsv\", collection_cleaned)\n","\n","# Convert collection df to JSON file for Anserini's document indexer\n","collection_to_json(\"retriever/collection_json/docs.json\", \"retriever/collection_cleaned.tsv\")"],"metadata":{"id":"MUCwxaEFAd0H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["processed_answers = process_answers(collection_cleaned)\n","processed_questions = process_questions(queries)"],"metadata":{"id":"epp_ysv6Cygj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word2index, word2count = create_vocab(processed_answers, processed_questions)\n","\n","print(\"Vocab size: {}\".format(len(word2index)))\n","print(\"Top {} common words: {}\".format(35, Counter(word2count).most_common(35)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MzDxFMF0C2oi","executionInfo":{"status":"ok","timestamp":1656723901617,"user_tz":240,"elapsed":6461,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"}},"outputId":"2ec8a1fd-7772-40df-81fc-aad231e6622c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocab size: 85034\n","Top 35 common words: [('the', 371203), ('to', 233559), ('a', 201620), ('you', 166702), ('and', 163066), ('of', 157574), ('is', 129894), ('in', 120019), ('that', 111416), ('for', 89366), ('it', 83822), ('i', 74100), ('your', 68153), ('are', 67255), ('if', 60689), ('be', 59266), ('on', 58382), ('have', 55754), ('as', 50088), ('this', 49868), ('not', 49227), ('or', 46080), ('with', 45894), ('they', 44485), ('but', 41690), ('can', 38863), ('will', 36865), ('at', 35548), ('an', 31392), ('money', 31003), ('so', 29980), ('$', 29096), ('would', 28750), ('from', 28582), ('more', 27378)]\n"]}]},{"cell_type":"code","source":["qid_to_text, docid_to_text = id_to_text(collection, queries)\n","qid_to_tokenized_text, docid_to_tokenized_text = id_to_tokenized_text(processed_answers, processed_questions)"],"metadata":{"id":"ZGOGzYQ7DKfo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install pydot --quiet\n","!pip install gensim==3.8.3 --quiet\n","!pip install tensorflow-datasets --quiet\n","!pip install -U tensorflow-text==2.8.2 --quiet\n","!pip install transformers --quiet\n","!pip install pydot --quiet"],"metadata":{"id":"zp1_18muEEXD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","from tensorflow.keras.layers import Embedding, Input, Dense, Lambda\n","from tensorflow.keras.models import Model\n","import tensorflow.keras.backend as K\n","import tensorflow_datasets as tfds\n","import tensorflow_text as tf_text\n","\n","from transformers import BertTokenizer, TFBertModel\n","\n","\n","import sklearn as sk\n","import nltk\n","from nltk.corpus import reuters\n","from nltk.data import find\n","\n","import matplotlib.pyplot as plt\n","\n","import re"],"metadata":{"id":"A7jn7mzaFZ0J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"],"metadata":{"id":"iC4drcIUEFz-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"swaRze44FY-L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","\n","#bert_model = TFBertModel.from_pretrained('bert-base-cased')\n","\n","def generate_bert_tokens(data):\n","  input_ids = []\n","  token_type_ids = []\n","  att_masks = []\n","  labels = []\n","\n","\n","  #for i, seq in enumerate(tqdm(train_set)):\n","  for i, seq in enumerate(data):\n","    qid, ans_labels, cands = seq[0], seq[1], seq[2]\n","    #print (qid, ans_labels, cands)\n","    \n","    # Map question id to text\n","    q_text = qid_to_text[qid]\n","\n","    max_width = -1\n","\n","    # For each answer in the candidates\n","    for docid in cands:\n","      # Map the docid to text\n","      ans_text = docid_to_text[docid] if docid_to_text.get(docid) else ''\n","\n","      # Encode the sequence using BERT tokenizer\n","      encoded_seq = bert_tokenizer.encode_plus(q_text, ans_text[:500],\n","                                              max_length=512,\n","                                              pad_to_max_length=True,\n","                                              return_token_type_ids=True,\n","                                              truncation=True,\n","                                              return_attention_mask = True)\n","\n","      #print(q_text, len(ans_text))\n","\n","      # Get parameters\n","      input_ids.append(encoded_seq['input_ids'])\n","      token_type_ids.append(encoded_seq['token_type_ids'])\n","      att_masks.append(encoded_seq['attention_mask'])\n","\n","      # If an answer is in the list of relevant answers assign\n","      # positive label\n","      labels.append(1 if docid in ans_labels else 0)\n","\n","\n","      #max_width = max(max_width, len(ans_text+q_text))\n","      #print(max_width)\n","  return input_ids, token_type_ids, att_masks, labels"],"metadata":{"id":"VgT0QbGg1dfb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_ids, token_type_ids, att_masks, labels = generate_bert_tokens(train_set[:1000])\n","save_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/' + \"input_ids1.pickle\", input_ids)\n","save_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/' + \"token_type_ids1.pickle\", token_type_ids)\n","save_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/' + \"att_masks1.pickle\", att_masks)\n","save_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/' + \"labels1.pickle\", labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7lFv3bz4uB6C","executionInfo":{"status":"ok","timestamp":1656724461539,"user_tz":240,"elapsed":128353,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"}},"outputId":"aa3f5c6b-00a3-4483-9928-cd95be59c487"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["input_ids, token_type_ids, att_masks = generate_bert_tokens(train_set[1000:2000])\n","save_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/' + \"input_ids2.pickle\", input_ids)\n","save_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/' + \"token_type_ids2.pickle\", token_type_ids)\n","save_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/' + \"att_masks2.pickle\", att_masks)"],"metadata":{"id":"D0lpaXceoycW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_ids, token_type_ids, att_masks = generate_bert_tokens(train_set[2000:3000])\n","save_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/' + \"input_ids3.pickle\", input_ids)\n","save_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/' + \"token_type_ids3.pickle\", token_type_ids)\n","save_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/' + \"att_masks3.pickle\", att_masks)"],"metadata":{"id":"OSLsVZP7tGN_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_ids, token_type_ids, att_masks = generate_bert_tokens(train_set[3000:4000])\n","save_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/' + \"input_ids4.pickle\", input_ids)\n","save_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/' + \"token_type_ids4.pickle\", token_type_ids)\n","save_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/' + \"att_masks4.pickle\", att_masks)"],"metadata":{"id":"Eq57viKLyD4-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_ids, token_type_ids, att_masks = generate_bert_tokens(train_set[4000:])\n","save_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/' + \"input_ids5.pickle\", input_ids)\n","save_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/' + \"token_type_ids5.pickle\", token_type_ids)\n","save_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/' + \"att_masks5.pickle\", att_masks)"],"metadata":{"id":"ynPNjTFpA9oX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/' + \"input_ids5.pickle\", input_ids)\n","save_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/' + \"token_type_ids5.pickle\", token_type_ids)\n","save_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/' + \"att_masks5.pickle\", att_masks)"],"metadata":{"id":"c2Se0WcC2_VI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["id1 = load_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/' + \"input_ids1.pickle\")\n","type1 = load_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/' + \"token_type_ids1.pickle\")\n","mask1 = load_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/' + \"att_masks1.pickle\")\n","label1 = load_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/' + \"labels1.pickle\")\n","print(len(id1), len(type1), len(mask1), len(label1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2IPr2b_ejFC2","executionInfo":{"status":"ok","timestamp":1656724518702,"user_tz":240,"elapsed":32588,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"}},"outputId":"dd678d5b-5835-4b34-dfc9-deae69b0a254"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["49979 49979 49979 49979\n"]}]},{"cell_type":"code","source":["save_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/' + \"train_set.pickle\", train_set)\n","save_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/' + \"valid_set.pickle\", valid_set)\n","save_pickle('/gdrive/MyDrive/nlp-yuan_code/FinBERT-QA/data/data_pickle/' + \"test_set.pickle\", test_set)"],"metadata":{"id":"fQqJ0DTvoOjr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#train_dataloader = get_dataloader(train_set, \"train\")"],"metadata":{"id":"PTvTLz_swERC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Question id and Question text\n","#query_path = \"/gdrive/MyDrive/nlp-data/nlp-qa-datasets/FiQA/FiQA_train_task2/FiQA_train_question_final.tsv\"\n","#queries = load_questions_to_df(query_path)\n","# Question id and Answer id pair\n","#labels_path = \"/gdrive/MyDrive/nlp-data/nlp-qa-datasets/FiQA/FiQA_train_task2/FiQA_train_question_doc_final.tsv\"\n","#qid_docid = load_qid_docid_to_df(labels_path)\n","# qid to docid label map\n","#labels = label_to_dict(qid_docid)\n","#train_label, test_label, valid_label = split_label(qid_docid)\n","# Split Questions\n","#train_questions, test_questions, \\\n","#valid_questions = split_question(train_label, test_label, valid_label, queries)"],"metadata":{"id":"3g-w6-BdlprA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_bert_model():\n","    \"\"\"\n","    Build a simple classification model with BERT. Use the CLS Token output for classification purposes.\n","    \"\"\"\n","    \n","    ### YOUR CODE HERE\n","    hidden_size = 100\n","    dropout = 0.3\n","    learning_rate= 0.00005\n","    bert_model = TFBertModel.from_pretrained('bert-base-cased')\n","\n","    #restrict training to the train_layers outer transformer layers\n","    input_ids = tf.keras.layers.Input(shape=(512,), dtype=tf.int64, name='input_ids_layer') #--SOLUTION--\n","    token_type_ids = tf.keras.layers.Input(shape=(512,), dtype=tf.int64, name='token_type_ids_layer')\n","    attention_mask = tf.keras.layers.Input(shape=(512,), dtype=tf.int64, name='attention_mask_layer')\n","\n","    bert_inputs = {'input_ids': input_ids,\n","                   'token_type_ids': token_type_ids,\n","                   'attention_mask': attention_mask}         \n","\n","    bert_out = bert_model(bert_inputs) \n","\n","    cls_token = bert_out[0][:,0,:]\n","    #cls_token = bert_out[1]\n","\n","    hidden = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer')(cls_token)\n","    hidden = tf.keras.layers.Dropout(dropout)(hidden)  \n","\n","    classification = tf.keras.layers.Dense(1, activation='sigmoid',name='classification_layer')(hidden)\n","\n","    \n","    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=[classification])\n","    \n","    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n","                            loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \n","                            metrics='accuracy') \n","    ### END YOUR CODE\n","    \n","    return classification_model"],"metadata":{"id":"58YNBDglqlQK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = create_bert_model()"],"metadata":{"id":"ULaWJZqSNsry","executionInfo":{"status":"ok","timestamp":1656724593095,"user_tz":240,"elapsed":5441,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8d455a39-9985-472f-9588-ecaec0a573b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]}]},{"cell_type":"code","source":["del input_ids\n","del token_type_ids\n","del att_masks\n","del labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":218},"id":"7TgYR02g0GHs","executionInfo":{"status":"error","timestamp":1656724009426,"user_tz":240,"elapsed":584,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"}},"outputId":"c29b86ba-fb76-4951-cddf-539ab557dd89"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-9eb7188bbad5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0matt_masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'input_ids' is not defined"]}]},{"cell_type":"code","source":["n = len(id1[0])\n","for i in range(len(id1)):\n","  if len(id1[i]) != n:\n","    print(i, len(id1[i]))\n","    break\n","print(n)\n","\n","n = len(type1[0])\n","for i in range(len(type1)):\n","  if len(type1[i]) != n:\n","    print(i, len(type1[i]))\n","    break\n","print(n)\n","\n","n = len(mask1[0])\n","for i in range(len(mask1)):\n","  if len(mask1[i]) != n:\n","    print(i, len(mask1[i]))\n","    break\n","print(n)\n","\n","n = len(label1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OMZWLjf-7GQ_","executionInfo":{"status":"ok","timestamp":1656723116708,"user_tz":240,"elapsed":93,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"}},"outputId":"6f402ef7-620e-4b88-9cfe-c4a666063274"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2500\n","2500\n","2500\n"]}]},{"cell_type":"code","source":["bert_avg_model_history = model.fit([np.array(id1[:100]), np.array(type1[:100]), np.array(mask1[:100])],np.array(label1[:100]),\n","                                   validation_data=([np.array(id1[:10]), np.array(type1[:10]), np.array(mask1[:10])],np.array(label1[:10])),\n","                                   batch_size=8,epochs=1)  "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bBI4BglBmiQ8","executionInfo":{"status":"ok","timestamp":1656724977564,"user_tz":240,"elapsed":381095,"user":{"displayName":"jagannathan Lakshmipathy","userId":"05639691877293412421"}},"outputId":"f70e9cd8-e746-4b01-bd77-46d56063d422"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n","13/13 [==============================] - 381s 27s/step - loss: 0.2313 - accuracy: 0.9100 - val_loss: 0.0151 - val_accuracy: 1.0000\n"]}]}]}